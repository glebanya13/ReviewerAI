import pandas as pd
import numpy as np
import re
import pickle
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import seaborn as sns
import os

class F1ScoreCallback(Callback):
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π callback –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è F1-score"""
    def __init__(self, validation_data):
        super().__init__()
        self.validation_data = validation_data
        self.best_f1 = 0.0
        self.best_accuracy = 0.0
        self.best_val_accuracy = 0.0
        self.best_loss = float('inf')
        self.best_val_loss = float('inf')
        
    def on_epoch_end(self, epoch, logs=None):
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        X_val, y_val = self.validation_data
        y_pred_proba = self.model.predict(X_val, verbose=0)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        # –í—ã—á–∏—Å–ª—è–µ–º F1-score
        f1 = f1_score(y_val, y_pred, average='weighted')
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        current_accuracy = logs.get('accuracy', 0)
        current_val_accuracy = logs.get('val_accuracy', 0)
        current_loss = logs.get('loss', float('inf'))
        current_val_loss = logs.get('val_loss', float('inf'))
        
        improved = False
        
        if f1 > self.best_f1:
            self.best_f1 = f1
            improved = True
            
        if current_accuracy > self.best_accuracy:
            self.best_accuracy = current_accuracy
            improved = True
            
        if current_val_accuracy > self.best_val_accuracy:
            self.best_val_accuracy = current_val_accuracy
            improved = True
            
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            improved = True
            
        if current_val_loss < self.best_val_loss:
            self.best_val_loss = current_val_loss
            improved = True
        
        # –î–æ–±–∞–≤–ª—è–µ–º F1-score –≤ –ª–æ–≥–∏
        logs['f1_score'] = f1
        logs['best_f1'] = self.best_f1
        logs['best_accuracy'] = self.best_accuracy
        logs['best_val_accuracy'] = self.best_val_accuracy
        logs['best_loss'] = self.best_loss
        logs['best_val_loss'] = self.best_val_loss
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ–Ω–∞ —É–ª—É—á—à–∏–ª–∞—Å—å
        if improved:
            self.model.save('model_tf_long.h5')
            with open('tokenizer.pkl', 'wb') as f:
                pickle.dump(self.tokenizer, f)
            print(f"\nüéâ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞! F1: {f1:.4f}")
        
        print(f"\nüìä –≠–ø–æ—Ö–∞ {epoch + 1} - –ú–µ—Ç—Ä–∏–∫–∏:")
        print(f"   Accuracy: {current_accuracy:.4f} (–ª—É—á—à–∞—è: {self.best_accuracy:.4f})")
        print(f"   Val Accuracy: {current_val_accuracy:.4f} (–ª—É—á—à–∞—è: {self.best_val_accuracy:.4f})")
        print(f"   Loss: {current_loss:.4f} (–ª—É—á—à–∞—è: {self.best_loss:.4f})")
        print(f"   Val Loss: {current_val_loss:.4f} (–ª—É—á—à–∞—è: {self.best_val_loss:.4f})")
        print(f"   F1-Score: {f1:.4f} (–ª—É—á—à–∞—è: {self.best_f1:.4f})")

class ToxicityClassifier:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.max_features = 50000  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
        self.max_length = 300      # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.embedding_dim = 256  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        
    def preprocess_text(self, text):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if pd.isna(text):
            return ""
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = str(text).lower()
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'[^–∞-—è—ëa-z0-9\s]', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def load_data(self, csv_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")
        df = pd.read_csv(csv_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
        print(f"–ö–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {list(df.columns)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        if 'text' not in df.columns or 'label' not in df.columns:
            raise ValueError("–í –¥–∞—Ç–∞—Å–µ—Ç–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 'text' –∏ 'label'")
        
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        print(df['label'].value_counts())
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        print("–ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç...")
        df['text_processed'] = df['text'].apply(self.preprocess_text)
        
        # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã
        df = df[df['text_processed'].str.len() > 0]
        
        print(f"–ü–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å {len(df)} –∑–∞–ø–∏—Å–µ–π")
        
        return df
    
    def prepare_data(self, df):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        print("–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –º–µ—Ç–∫–∏
        X = df['text_processed'].values
        y = df['label'].values
        
        # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        self.tokenizer = Tokenizer(num_words=self.max_features, oov_token='<OOV>')
        self.tokenizer.fit_on_texts(X)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        X_sequences = self.tokenizer.texts_to_sequences(X)
        X_padded = pad_sequences(X_sequences, maxlen=self.max_length, padding='post', truncating='post')
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏
        X_train, X_test, y_train, y_test = train_test_split(
            X_padded, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"–†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape}")
        print(f"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape}")
        print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(self.tokenizer.word_index)}")
        
        return X_train, X_test, y_train, y_test
    
    def build_model(self, vocab_size):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Å ~20M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        print("–°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å...")
        print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")
        print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {self.max_length}")
        print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {self.embedding_dim}")
        
        model = Sequential([
            # –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–π - —ç—Ç–æ —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π —Å–ª–æ–π –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
            Embedding(vocab_size, self.embedding_dim, input_length=self.max_length, name='embedding'),
            
            # –ü–µ—Ä–≤—ã–π –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π LSTM
            Bidirectional(LSTM(512, return_sequences=True, dropout=0.3, recurrent_dropout=0.3), name='bidirectional_lstm_1'),
            
            # –í—Ç–æ—Ä–æ–π –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π LSTM
            Bidirectional(LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.3), name='bidirectional_lstm_2'),
            
            # –¢—Ä–µ—Ç–∏–π LSTM
            LSTM(128, dropout=0.3, recurrent_dropout=0.3, name='lstm_3'),
            
            # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
            Dense(1024, activation='relu', name='dense_1'),
            Dropout(0.5),
            
            Dense(512, activation='relu', name='dense_2'),
            Dropout(0.4),
            
            Dense(256, activation='relu', name='dense_3'),
            Dropout(0.3),
            
            Dense(128, activation='relu', name='dense_4'),
            Dropout(0.2),
            
            # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
            Dense(1, activation='sigmoid', name='output')
        ])
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º Adam
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        print("–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
        model.summary()
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        # –°–Ω–∞—á–∞–ª–∞ —Å—Ç—Ä–æ–∏–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∏–º–µ—Ä–æ–º –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        print("\nüîß –°—Ç—Ä–æ–∏–º –º–æ–¥–µ–ª—å...")
        dummy_input = tf.random.normal((1, self.max_length))
        _ = model(dummy_input)
        
        total_params = model.count_params()
        print(f"\nüî¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
        print(f"üìä –ü—Ä–∏–º–µ—Ä–Ω–æ {total_params/1_000_000:.1f}M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø–æ —Å–ª–æ—è–º
        print("\nüìã –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Å–ª–æ—è–º:")
        for i, layer in enumerate(model.layers):
            layer_params = layer.count_params()
            print(f"  –°–ª–æ–π {i+1} ({layer.name}): {layer_params:,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
            if layer_params == 0:
                print(f"    ‚ö†Ô∏è  –°–ª–æ–π {layer.name} –∏–º–µ–µ—Ç 0 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤!")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –µ—Å—Ç—å
        if total_params == 0:
            print("\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ä–∞–≤–Ω–æ 0! –í–æ–∑–º–æ–∂–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –º–æ–¥–µ–ª–∏.")
            print("üîç –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
            print("   - –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è")
            print("   - –ü—Ä–æ–±–ª–µ–º—ã —Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π —Å–ª–æ–µ–≤")
            print("   - –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏")
        elif total_params < 1_000_000:
            print("\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–µ–Ω—å—à–µ 1M. –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–æ–π.")
        else:
            print("\n‚úÖ –ú–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á.")
        
        return model
    
    def train(self, X_train, y_train, X_test, y_test):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        print("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
        
        vocab_size = len(self.tokenizer.word_index) + 1
        self.model = self.build_model(vocab_size)
        
        # –°–æ–∑–¥–∞–µ–º callback –¥–ª—è F1-score
        f1_callback = F1ScoreCallback((X_test, y_test))
        f1_callback.tokenizer = self.tokenizer
        
        # Callbacks –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=1
        )
        
        model_checkpoint = ModelCheckpoint(
            'model_tf_long.h5',
            monitor='val_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
        )
        
        # –û–±—É—á–µ–Ω–∏–µ
        print("\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        history = self.model.fit(
            X_train, y_train,
            epochs=20,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            batch_size=64,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            validation_data=(X_test, y_test),
            callbacks=[early_stopping, model_checkpoint, f1_callback],
            verbose=1
        )
        
        return history
    
    def evaluate(self, X_test, y_test):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
        print("–û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å...")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_proba = self.model.predict(X_test)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")
        print(f"F1-Score: {f1:.4f}")
        
        print("\n–û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:")
        print(classification_report(y_test, y_pred, target_names=['–ù–µ—Ç–æ–∫—Å–∏—á–Ω–æ', '–¢–æ–∫—Å–∏—á–Ω–æ']))
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=['–ù–µ—Ç–æ–∫—Å–∏—á–Ω–æ', '–¢–æ–∫—Å–∏—á–Ω–æ'],
                    yticklabels=['–ù–µ—Ç–æ–∫—Å–∏—á–Ω–æ', '–¢–æ–∫—Å–∏—á–Ω–æ'])
        plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')
        plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
        plt.savefig('confusion_matrix_tf_long.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return accuracy, y_pred
    
    def predict_toxicity(self, text):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        if self.model is None or self.tokenizer is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ train()")
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        processed_text = self.preprocess_text(text)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        sequence = self.tokenizer.texts_to_sequences([processed_text])
        padded_sequence = pad_sequences(sequence, maxlen=self.max_length, padding='post', truncating='post')
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        probability = self.model.predict(padded_sequence, verbose=0)[0][0]
        prediction = 1 if probability > 0.5 else 0
        
        return {
            'text': text,
            'processed_text': processed_text,
            'is_toxic': bool(prediction),
            'toxicity_probability': float(probability),
            'confidence': float(max(probability, 1 - probability))
        }
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if self.model is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å Keras
        self.model.save('model_tf_long.h5')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        with open('tokenizer.pkl', 'wb') as f:
            pickle.dump(self.tokenizer, f)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: model_tf_long.h5")
        print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: tokenizer.pkl")
    
    def load_model(self, model_path='model_tf_long.h5', tokenizer_path='tokenizer.pkl'):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            self.model = tf.keras.models.load_model(model_path)
            with open(tokenizer_path, 'rb') as f:
                self.tokenizer = pickle.load(f)
            print("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    print("üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    classifier = ToxicityClassifier()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = classifier.load_data('C:/Andrey/Study/5 —Å–µ–º/HAKATON/HAKATON/dataset/train_final_complete.csv')
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    X_train, X_test, y_train, y_test = classifier.prepare_data(df)
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    history = classifier.train(X_train, y_train, X_test, y_test)
    
    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å
    accuracy, predictions = classifier.evaluate(X_test, y_test)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    classifier.save_model()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
    print("\n" + "="*50)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò –ù–ê –ü–†–ò–ú–ï–†–ê–•")
    print("="*50)
    
    test_texts = [
        "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?",
        "–¢—ã –¥–µ–±–∏–ª –∏ –∏–¥–∏–æ—Ç!",
        "–°–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–º–æ—â—å",
        "–£–±–∏–π—Ü–∞ –∏ –º–∞–Ω—å—è–∫!",
        "–•–æ—Ä–æ—à–∞—è –ø–æ–≥–æ–¥–∞ —Å–µ–≥–æ–¥–Ω—è"
    ]
    
    for text in test_texts:
        result = classifier.predict_toxicity(text)
        print(f"\n–¢–µ–∫—Å—Ç: '{result['text']}'")
        print(f"–¢–æ–∫—Å–∏—á–Ω–æ: {result['is_toxic']}")
        print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏: {result['toxicity_probability']:.3f}")
        print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.3f}")

if __name__ == "__main__":
    main()
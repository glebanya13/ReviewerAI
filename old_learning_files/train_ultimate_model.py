import pandas as pd
import numpy as np
import time
import pickle
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_recall_curve, roc_auc_score
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.combine import SMOTETomek
import psutil
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# TensorFlow —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Concatenate, Add
from tensorflow.keras.optimizers import Adam, AdamW
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
try:
    from tensorflow.keras.callbacks import CyclicLR
except ImportError:
    # –ï—Å–ª–∏ CyclicLR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
    class CyclicLR:
        def __init__(self, *args, **kwargs):
            pass
from tensorflow.keras.mixed_precision import set_global_policy
from tensorflow.keras.regularizers import l1_l2
from tensorflow.keras import backend as K
from tensorflow.keras.utils import plot_model

class UltimateModelTrainer:
    """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏"""

    def __init__(self):
        self.setup_gpu()
        self.setup_mixed_precision()
        self.vectorizer = None
        self.model = None
        self.history = None
        self.results = {}

    def setup_gpu(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        print("üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤...")

        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è CPU
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
        os.environ['OMP_NUM_THREADS'] = str(psutil.cpu_count())
        os.environ['TF_NUM_INTEROP_THREADS'] = str(psutil.cpu_count())
        os.environ['TF_NUM_INTRAOP_THREADS'] = str(psutil.cpu_count())

        # GPU –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            try:
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                print(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ GPU: {len(gpus)}")
            except RuntimeError as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ GPU: {e}")
        else:
            print("‚ö†Ô∏è GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π CPU")

        tf.config.threading.set_inter_op_parallelism_threads(psutil.cpu_count())
        tf.config.threading.set_intra_op_parallelism_threads(psutil.cpu_count())

        print(f"‚úÖ CPU —è–¥–µ—Ä: {psutil.cpu_count()}")
        print(f"‚úÖ RAM: {psutil.virtual_memory().total / (1024**3):.1f} –ì–ë")

    def setup_mixed_precision(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏"""
        try:
            set_global_policy('mixed_float16')
            print("‚úÖ –í–∫–ª—é—á–µ–Ω–∞ —Å–º–µ—à–∞–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (mixed_float16)")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å: {e}")

    def load_combined_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏"""
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        try:
            df1 = pd.read_csv('../dataset/train_final_complete.csv')
            print(f"   train_final_complete.csv: {df1.shape}")
        except:
            df1 = pd.read_csv('train_final_complete.csv')
            print(f"   train_final_complete.csv: {df1.shape}")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        try:
            df2 = pd.read_csv('../dataset/newtrain.csv')
            print(f"   newtrain.csv: {df2.shape}")
        except:
            df2 = pd.read_csv('newtrain.csv')
            print(f"   newtrain.csv: {df2.shape}")

        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        combined_df = pd.concat([df1, df2], ignore_index=True)
        print(f"   –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {combined_df.shape}")

        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        combined_df = combined_df.dropna(subset=['text'])
        combined_df = combined_df.drop_duplicates(subset=['text'])
        combined_df['label'] = combined_df['label'].astype(int)

        print(f"   –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {combined_df.shape}")
        print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {combined_df['label'].value_counts().to_dict()}")

        X = combined_df['text'].astype(str)
        y = combined_df['label'].astype(int)

        return X, y

    def create_advanced_tfidf_features(self, X_train, X_test, max_features=10000):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        print("üî§ –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        self.vectorizer = TfidfVectorizer(
            max_features=max_features,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            ngram_range=(1, 2),  # –£–±—Ä–∞–ª —Ç—Ä–∏–≥—Ä–∞–º–º—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            min_df=3,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
            max_df=0.9,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤
            sublinear_tf=True,
            use_idf=True,
            smooth_idf=True,
            norm='l2',
            stop_words=None,
            dtype=np.float32
        )

        print("   –û–±—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞...")
        X_train_tfidf = self.vectorizer.fit_transform(X_train)
        X_test_tfidf = self.vectorizer.transform(X_test)

        print(f"   –†–∞–∑–º–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞: {X_train_tfidf.shape[1]}")

        # –†–∞—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü
        memory_size = (X_train_tfidf.data.nbytes + X_train_tfidf.indices.nbytes + X_train_tfidf.indptr.nbytes) / (1024**3)
        print(f"   –†–∞–∑–º–µ—Ä –≤ –ø–∞–º—è—Ç–∏: {memory_size:.2f} –ì–ë")

        return X_train_tfidf, X_test_tfidf

    def apply_advanced_sampling(self, X_train, y_train):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –ø–∞–º—è—Ç–∏"""
        print("‚öñÔ∏è –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è...")

        unique, counts = np.unique(y_train, return_counts=True)
        print(f"   –î–æ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è: {dict(zip(unique, counts))}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å
        available_memory = psutil.virtual_memory().available / (1024**3)  # –ì–ë
        required_memory = X_train.shape[0] * X_train.shape[1] * 4 / (1024**3)  # float32

        print(f"   –î–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å: {available_memory:.1f} –ì–ë")
        print(f"   –¢—Ä–µ–±—É–µ–º–∞—è –ø–∞–º—è—Ç—å: {required_memory:.1f} –ì–ë")

        if required_memory > available_memory * 0.5:  # –ï—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–µ 50% –ø–∞–º—è—Ç–∏
            print("   ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è SMOTE, –∏—Å–ø–æ–ª—å–∑—É–µ–º class_weight")
            return X_train.toarray() if hasattr(X_train, 'toarray') else X_train, y_train

        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        print("   –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç...")
        X_train_dense = X_train.toarray() if hasattr(X_train, 'toarray') else X_train

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π SMOTE —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        try:
            sampler = SMOTE(
                k_neighbors=3,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                random_state=42
            )

            X_train_balanced, y_train_balanced = sampler.fit_resample(X_train_dense, y_train)

            unique, counts = np.unique(y_train_balanced, return_counts=True)
            print(f"   –ü–æ—Å–ª–µ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è: {dict(zip(unique, counts))}")

            return X_train_balanced, y_train_balanced

        except MemoryError:
            print("   ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è SMOTE, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
            return X_train_dense, y_train

    def create_ultimate_model(self, input_dim):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è –≤—ã—Å–æ–∫–æ–≥–æ F1-score"""
        print("üß† –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")

        model = Sequential([
            Input(shape=(input_dim,), name='input_layer'),

            # –ü–µ—Ä–≤—ã–π –±–ª–æ–∫ - –±–æ–ª–µ–µ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π
            Dense(1024, activation='relu', kernel_regularizer=l1_l2(l1=0.0001, l2=0.001)),
            BatchNormalization(),
            Dropout(0.3),

            # –í—Ç–æ—Ä–æ–π –±–ª–æ–∫
            Dense(512, activation='relu', kernel_regularizer=l1_l2(l1=0.0001, l2=0.001)),
            BatchNormalization(),
            Dropout(0.4),

            # –¢—Ä–µ—Ç–∏–π –±–ª–æ–∫
            Dense(256, activation='relu', kernel_regularizer=l1_l2(l1=0.0001, l2=0.001)),
            BatchNormalization(),
            Dropout(0.3),

            # –ß–µ—Ç–≤–µ—Ä—Ç—ã–π –±–ª–æ–∫
            Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.0001, l2=0.001)),
            BatchNormalization(),
            Dropout(0.2),

            # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
            Dense(1, activation='sigmoid', dtype='float32', name='output')
        ])

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä Adam —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è –≤—ã—Å–æ–∫–æ–≥–æ F1-score
        optimizer = Adam(
            learning_rate=0.001,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-07,
            clipnorm=1.0
        )

        model.compile(
            optimizer=optimizer,
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )

        print("   –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
        model.summary()

        return model

    def create_advanced_callbacks(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö callback'–æ–≤"""
        callbacks = [
            EarlyStopping(
                monitor='val_accuracy',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –º–µ—Ç—Ä–∏–∫—É
                patience=15,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                restore_best_weights=True,
                verbose=1,
                mode='max'
            ),
            ReduceLROnPlateau(
                monitor='val_accuracy',
                factor=0.5,
                patience=7,
                min_lr=1e-8,
                verbose=1,
                mode='max'
            ),
            ModelCheckpoint(
                filepath='../model_tf_3_best.h5',
                monitor='val_accuracy',
                save_best_only=True,
                save_weights_only=False,
                verbose=1,
                mode='max'
            )
        ]

        return callbacks

    def f1_score_metric(self, y_true, y_pred):
        """–ö–∞—Å—Ç–æ–º–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ F1-score –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –æ–¥–Ω–æ–º—É —Ç–∏–ø—É –¥–∞–Ω–Ω—ã—Ö
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)

        def recall(y_true, y_pred):
            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
            possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
            recall = true_positives / (possible_positives + K.epsilon())
            return recall

        def precision(y_true, y_pred):
            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
            precision = true_positives / (predicted_positives + K.epsilon())
            return precision

        precision_val = precision(y_true, y_pred)
        recall_val = recall(y_true, y_pred)
        return 2*((precision_val*recall_val)/(precision_val+recall_val+K.epsilon()))

    def train_ultimate_model(self, X_train, y_train, X_val, y_val):
        """–û–±—É—á–µ–Ω–∏–µ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        print("üèãÔ∏è –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏...")

        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model = self.create_ultimate_model(X_train.shape[1])

        # Callback'–∏
        callbacks = self.create_advanced_callbacks()

        start_time = time.time()

        # –û–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è F1-score
        self.history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=100,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            batch_size=512,  # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            callbacks=callbacks,
            verbose=1,
            shuffle=True,
            class_weight={0: 1.0, 1: 3.0}  # –ë–æ–ª—å—à–∏–π –≤–µ—Å —Ç–æ–∫—Å–∏—á–Ω—ã–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º
        )

        training_time = time.time() - start_time
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.2f} —Å–µ–∫—É–Ω–¥")

        return training_time

    def evaluate_ultimate_model(self, X_test, y_test):
        """–û—Ü–µ–Ω–∫–∞ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        print("üìä –û—Ü–µ–Ω–∫–∞ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏...")

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
        y_pred_proba = self.model.predict(X_test, batch_size=512, verbose=1)

        # –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏–∏ F1-score
        thresholds = np.arange(0.3, 0.8, 0.01)
        best_f1 = 0
        best_threshold = 0.5

        for threshold in thresholds:
            y_pred_temp = (y_pred_proba > threshold).astype(int).flatten()
            f1_temp = f1_score(y_test, y_pred_temp)
            if f1_temp > best_f1:
                best_f1 = f1_temp
                best_threshold = threshold

        print(f"   –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {best_threshold:.3f}")

        y_pred = (y_pred_proba > best_threshold).astype(int).flatten()

        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_pred_proba)

        self.results = {
            'accuracy': accuracy,
            'f1_score': f1,
            'roc_auc': roc_auc,
            'best_threshold': best_threshold,
            'classification_report': classification_report(y_test, y_pred),
            'confusion_matrix': confusion_matrix(y_test, y_pred),
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }

        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")
        print(f"‚úÖ F1-score: {f1:.4f}")
        print(f"‚úÖ ROC-AUC: {roc_auc:.4f}")

        return self.results

    def save_ultimate_model_and_results(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏...")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model.save('model_tf_3.h5')
        print("   ‚úÖ –£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: model_tf_3.h5")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
        with open('../tokenizer_tf_3.pkl', 'wb') as f:
            pickle.dump(self.vectorizer, f)
        print("   ‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: tokenizer_tf_3.pkl")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open('../results_tf_3.pkl', 'wb') as f:
            pickle.dump(self.results, f)
        print("   ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: results_tf_3.pkl")

    def plot_ultimate_training_history(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        print("üìà –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏...")

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        # –¢–æ—á–Ω–æ—Å—Ç—å
        axes[0, 0].plot(self.history.history['accuracy'], label='–û–±—É—á–µ–Ω–∏–µ', linewidth=2)
        axes[0, 0].plot(self.history.history['val_accuracy'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è', linewidth=2)
        axes[0, 0].set_title('–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (Ultimate)', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[0, 0].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        axes[0, 1].plot(self.history.history['loss'], label='–û–±—É—á–µ–Ω–∏–µ', linewidth=2)
        axes[0, 1].plot(self.history.history['val_loss'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è', linewidth=2)
        axes[0, 1].set_title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (Ultimate)', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[0, 1].set_ylabel('–ü–æ—Ç–µ—Ä–∏')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # F1-score
        if 'f1_score_metric' in self.history.history:
            axes[0, 2].plot(self.history.history['f1_score_metric'], label='–û–±—É—á–µ–Ω–∏–µ', linewidth=2)
            axes[0, 2].plot(self.history.history['val_f1_score_metric'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è', linewidth=2)
            axes[0, 2].set_title('F1-Score (Ultimate)', fontsize=14, fontweight='bold')
            axes[0, 2].set_xlabel('–≠–ø–æ—Ö–∞')
            axes[0, 2].set_ylabel('F1-Score')
            axes[0, 2].legend()
            axes[0, 2].grid(True, alpha=0.3)

        # Precision
        if 'precision' in self.history.history:
            axes[1, 0].plot(self.history.history['precision'], label='–û–±—É—á–µ–Ω–∏–µ', linewidth=2)
            axes[1, 0].plot(self.history.history['val_precision'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è', linewidth=2)
            axes[1, 0].set_title('Precision (Ultimate)', fontsize=14, fontweight='bold')
            axes[1, 0].set_xlabel('–≠–ø–æ—Ö–∞')
            axes[1, 0].set_ylabel('Precision')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

        # Recall
        if 'recall' in self.history.history:
            axes[1, 1].plot(self.history.history['recall'], label='–û–±—É—á–µ–Ω–∏–µ', linewidth=2)
            axes[1, 1].plot(self.history.history['val_recall'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è', linewidth=2)
            axes[1, 1].set_title('Recall (Ultimate)', fontsize=14, fontweight='bold')
            axes[1, 1].set_xlabel('–≠–ø–æ—Ö–∞')
            axes[1, 1].set_ylabel('Recall')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)

        # Learning Rate
        if hasattr(self.history.history, 'lr'):
            axes[1, 2].plot(self.history.history['lr'], linewidth=2, color='red')
            axes[1, 2].set_title('Learning Rate (Ultimate)', fontsize=14, fontweight='bold')
            axes[1, 2].set_xlabel('–≠–ø–æ—Ö–∞')
            axes[1, 2].set_ylabel('Learning Rate')
            axes[1, 2].set_yscale('log')
            axes[1, 2].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('training_history_tf_3.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("   ‚úÖ –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω: training_history_tf_3.png")

    def plot_ultimate_confusion_matrix(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        plt.figure(figsize=(10, 8))
        cm = self.results['confusion_matrix']

        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

        # –°–æ–∑–¥–∞–Ω–∏–µ heatmap
        sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',
                   square=True, linewidths=0.5, cbar_kws={"shrink": .8})

        plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (Ultimate Model)\n–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø–æ —Å—Ç—Ä–æ–∫–∞–º',
                 fontsize=16, fontweight='bold', pad=20)
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', fontsize=12)
        plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', fontsize=12)

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –æ–±—Ä–∞–∑—Ü–æ–≤
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                plt.text(j+0.5, i+0.7, f'n={cm[i,j]}',
                        ha='center', va='center', fontsize=10, color='red')

        plt.savefig('confusion_matrix_tf_3.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("   ‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: confusion_matrix_tf_3.png")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï –£–õ–¨–¢–ò–ú–ê–¢–ò–í–ù–û–ô –ú–û–î–ï–õ–ò –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –¢–û–ö–°–ò–ß–ù–û–°–¢–ò")
    print("=" * 80)
    print(f"üìÖ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("üéØ –¶–µ–ª—å: F1-score ‚â• 0.95")
    print()

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = UltimateModelTrainer()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    X, y = trainer.load_combined_data()

    # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
    print("üîÑ –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.15, random_state=42, stratify=y  # –ú–µ–Ω—å—à–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö = –±–æ–ª—å—à–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    )

    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.15, random_state=42, stratify=y_train
    )

    print(f"   –û–±—É—á–µ–Ω–∏–µ: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)")
    print(f"   –í–∞–ª–∏–¥–∞—Ü–∏—è: {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%)")
    print(f"   –¢–µ—Å—Ç: {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)")

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_train_tfidf, X_val_tfidf = trainer.create_advanced_tfidf_features(X_train, X_val)
    X_test_tfidf = trainer.vectorizer.transform(X_test)

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    X_val_dense = X_val_tfidf.toarray()

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    X_train_balanced, y_train_balanced = trainer.apply_advanced_sampling(X_train_tfidf, y_train)

    # –û–±—É—á–µ–Ω–∏–µ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
    training_time = trainer.train_ultimate_model(X_train_balanced, y_train_balanced, X_val_dense, y_val)

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    X_test_dense = X_test_tfidf.toarray()

    # –û—Ü–µ–Ω–∫–∞ —É–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
    results = trainer.evaluate_ultimate_model(X_test_dense, y_test)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    trainer.save_ultimate_model_and_results()

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    trainer.plot_ultimate_training_history()
    trainer.plot_ultimate_confusion_matrix()

    # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "=" * 80)
    print("üéâ –û–ë–£–ß–ï–ù–ò–ï –£–õ–¨–¢–ò–ú–ê–¢–ò–í–ù–û–ô –ú–û–î–ï–õ–ò –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 80)
    print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time/60:.1f} –º–∏–Ω—É—Ç")
    print(f"üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {results['accuracy']:.4f}")
    print(f"üìä F1-score: {results['f1_score']:.4f}")
    print(f"üìà ROC-AUC: {results['roc_auc']:.4f}")
    print(f"üîç –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥: {results['best_threshold']:.3f}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏
    if results['f1_score'] >= 0.95:
        print("üèÜ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê: F1-score ‚â• 0.95!")
    elif results['f1_score'] >= 0.90:
        print("ü•à –û–¢–õ–ò–ß–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢: F1-score ‚â• 0.90!")
    elif results['f1_score'] >= 0.85:
        print("ü•â –•–û–†–û–®–ò–ô –†–ï–ó–£–õ–¨–¢–ê–¢: F1-score ‚â• 0.85!")
    else:
        print("‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")

    print("\nüìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print("   ‚Ä¢ model_tf_3.h5 - –£–ª—å—Ç–∏–º–∞—Ç–∏–≤–Ω–∞—è TensorFlow –º–æ–¥–µ–ª—å")
    print("   ‚Ä¢ tokenizer_tf_3.pkl - –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä")
    print("   ‚Ä¢ results_tf_3.pkl - –ü–æ–¥—Ä–æ–±–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è")
    print("   ‚Ä¢ confusion_matrix_tf_3.png - –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫")
    print("   ‚Ä¢ training_history_tf_3.png - –ü–æ–¥—Ä–æ–±–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è")
    print("=" * 80)

if __name__ == "__main__":
    main()

import pickle
import tensorflow as tf
import re
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

class ToxicityPredictor:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.max_length = 200
        
    def preprocess_text(self, text):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if pd.isna(text):
            return ""
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = str(text).lower()
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'[^–∞-—è—ëa-z0-9\s]', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def load_model(self, model_path='model_tf.h5', tokenizer_path='tokenizer_tf.pkl'):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            self.model = tf.keras.models.load_model(model_path)
            with open(tokenizer_path, 'rb') as f:
                self.tokenizer = pickle.load(f)
            print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def predict_toxicity(self, text):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        if self.model is None or self.tokenizer is None:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ load_model()")
            return None

        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        processed_text = self.preprocess_text(text)
        if not processed_text:
            print(f"‚ö†Ô∏è –ü–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π: '{text}'")
            return {
                'text': text,
                'processed_text': processed_text,
                'is_toxic': False,
                'toxicity_probability': 0.0,
                'confidence': 1.0,
                'error': '–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏'
            }

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        if hasattr(self.tokenizer, 'texts_to_sequences'):
            # Keras Tokenizer
            sequence = self.tokenizer.texts_to_sequences([processed_text])
            if not sequence or not any(sequence[0]):
                print(f"‚ö†Ô∏è –ü–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—É—Å—Ç–∞: '{processed_text}'")
                return {
                    'text': text,
                    'processed_text': processed_text,
                    'is_toxic': False,
                    'toxicity_probability': 0.0,
                    'confidence': 1.0,
                    'error': '–ü—É—Å—Ç–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏'
                }
            padded_sequence = pad_sequences(sequence, maxlen=self.max_length, padding='post', truncating='post')
            if padded_sequence.shape[1] == 0:
                print(f"‚ö†Ô∏è –ü–æ—Å–ª–µ pad_sequences –ø–æ–ª—É—á–µ–Ω–∞ –ø—É—Å—Ç–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: '{processed_text}'")
                return {
                    'text': text,
                    'processed_text': processed_text,
                    'is_toxic': False,
                    'toxicity_probability': 0.0,
                    'confidence': 1.0,
                    'error': '–ü—É—Å—Ç–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ pad_sequences'
                }
            input_data = padded_sequence
        elif hasattr(self.tokenizer, 'transform'):
            # TfidfVectorizer
            input_data = self.tokenizer.transform([processed_text]) if isinstance(processed_text, list) else self.tokenizer.transform([processed_text])
        else:
            print("‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞!")
            return None

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        try:
            probability = self.model.predict(input_data, verbose=0)[0][0]
            prediction = 1 if probability > 0.5 else 0
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
            return {
                'text': text,
                'processed_text': processed_text,
                'is_toxic': False,
                'toxicity_probability': 0.0,
                'confidence': 1.0,
                'error': str(e)
            }

        return {
            'text': text,
            'processed_text': processed_text,
            'is_toxic': bool(prediction),
            'toxicity_probability': probability,
            'confidence': max(probability, 1 - probability)
        }
    
    def predict_batch(self, texts):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        results = []
        for text in texts:
            result = self.predict_toxicity(text)
            if result:
                results.append(result)
        return results

def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å
    predictor = ToxicityPredictor()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    if not predictor.load_model():
        print("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å, –∑–∞–ø—É—Å—Ç–∏–≤ train.py")
        return
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
    test_texts = [
        "—Å—É–∫–∞–±–ª—è—Ç—å",
        "—É –¥–µ–Ω–∏—Å–∞ –º–∞–ª–µ–Ω—å–∫–∏–µ —Å–∏—Å—å–∫–∏",
        "—É –≥–ª–µ—é–∞—Å–∞ –º–∞–ª–µ–Ω—å–∫–∏–π —á–ª–µ–Ω",
        "–û—Ç–ª–∏—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç, –æ—á–µ–Ω—å –¥–æ–≤–æ–ª–µ–Ω!",
        "—É –∂–µ–Ω–∏ –±–æ–ª—å—à–∏–µ —Å–∏—Å—å–∫–∏ —á–µ–º —É –¥–µ–Ω–∏—Å–∞",
        "–¢—ã –¥–µ–±–∏–ª –∏ –∏–¥–∏–æ—Ç!",
        "–°–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–º–æ—â—å",
        "–£–±–∏–π—Ü–∞ –∏ –º–∞–Ω—å—è–∫!",
        "–•–æ—Ä–æ—à–∞—è –ø–æ–≥–æ–¥–∞ —Å–µ–≥–æ–¥–Ω—è",
        "–ò–¥–∏ –Ω–∞—Ö—É–π!",
        "–û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞!",
        "–°—É–∫–∞ –±–ª—è–¥—å!",
        "–°–Ω–∞—á–∞–ª–∞ –ø–æ–º–æ–≥–∏ —Å–µ–±–µ —Å–∞–º, –∞ –ø–æ—Ç–æ–º –¥—Ä—É–≥–∏–º. –ü–æ—ç—Ç–æ–º—É —è –±–ª –Ω–µ –ø–æ–º–æ–≥—É —Ç–µ–±–µ. –ü–æ–Ω—è–ª –º–µ–Ω—è, –ø–∞–¥–ª–∞?"
    ]
    
    print("\n" + "="*60)
    print("–ü–†–û–í–ï–†–ö–ê –¢–û–ö–°–ò–ß–ù–û–°–¢–ò –¢–ï–ö–°–¢–û–í")
    print("="*60)
    
    for text in test_texts:
        result = predictor.predict_toxicity(text)
        if result:
            status = "üî¥ –¢–û–ö–°–ò–ß–ù–û" if result['is_toxic'] else "üü¢ –ù–ï–¢–û–ö–°–ò–ß–ù–û"
            print(f"\n–¢–µ–∫—Å—Ç: '{result['text']}'")
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {status}")
            print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏: {result['toxicity_probability']}")
            print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']}")
            print("-" * 40)

if __name__ == "__main__":
    main()

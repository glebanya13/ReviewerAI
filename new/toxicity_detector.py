import pickle
import tensorflow as tf
import re
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

class ToxicityPredictor:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.max_length = 200
        
    def preprocess_text(self, text):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º None –∏ NaN
        if text is None or pd.isna(text):
            return ""
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É
        text = str(text)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
        if not text.strip():
            return ""
        
        try:
            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
            text = text.lower()
            
            # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É –±–æ–ª—å—à–µ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ª—É—á—à–µ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
            text = re.sub(r'[^–∞-—è—ëa-z0-9\s]', ' ', text)
            
            # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
            text = re.sub(r'\s+', ' ', text)
            
            # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
            text = text.strip()
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ—Ç—É
            if not text:
                return ""
                
            return text
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞ '{str(text)[:50]}...': {e}")
            return ""
    
    def load_model(self, model_path='model_tf.h5', tokenizer_path='tokenizer_tf.pkl'):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            self.model = tf.keras.models.load_model(model_path)
            with open(tokenizer_path, 'rb') as f:
                self.tokenizer = pickle.load(f)
            print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            return True
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def predict_toxicity(self, text):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        if self.model is None or self.tokenizer is None:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ load_model()")
            return None

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
        original_text = text
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        processed_text = self.preprocess_text(text)
        if not processed_text:
            print(f"‚ö†Ô∏è –ü–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π: '{text}'")
            return {
                'text': original_text,
                'processed_text': processed_text,
                'is_toxic': False,
                'toxicity_probability': 0.0,
                'confidence': 1.0,
                'error': '–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏'
            }

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        try:
            if hasattr(self.tokenizer, 'texts_to_sequences'):
                # Keras Tokenizer (—Å—Ç–∞—Ä—ã–π —Ç–∏–ø)
                sequence = self.tokenizer.texts_to_sequences([processed_text])
                if not sequence or not any(sequence[0]):
                    print(f"‚ö†Ô∏è –ü–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—É—Å—Ç–∞: '{processed_text}'")
                    return {
                        'text': original_text,
                        'processed_text': processed_text,
                        'is_toxic': False,
                        'toxicity_probability': 0.0,
                        'confidence': 1.0,
                        'error': '–ü—É—Å—Ç–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏'
                    }
                padded_sequence = pad_sequences(sequence, maxlen=self.max_length, padding='post', truncating='post')
                if padded_sequence.shape[1] == 0:
                    print(f"‚ö†Ô∏è –ü–æ—Å–ª–µ pad_sequences –ø–æ–ª—É—á–µ–Ω–∞ –ø—É—Å—Ç–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: '{processed_text}'")
                    return {
                        'text': original_text,
                        'processed_text': processed_text,
                        'is_toxic': False,
                        'toxicity_probability': 0.0,
                        'confidence': 1.0,
                        'error': '–ü—É—Å—Ç–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ pad_sequences'
                    }
                input_data = padded_sequence
                
            elif hasattr(self.tokenizer, 'transform'):
                # TfidfVectorizer
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –ø—É—Å—Ç–æ–π
                if not processed_text or not processed_text.strip():
                    return {
                        'text': original_text,
                        'processed_text': processed_text,
                        'is_toxic': False,
                        'toxicity_probability': 0.0,
                        'confidence': 1.0,
                        'error': '–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è TF-IDF'
                    }
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ TF-IDF –≤–µ–∫—Ç–æ—Ä
                input_data = self.tokenizer.transform([processed_text])
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
                if input_data.shape[1] == 0:
                    return {
                        'text': original_text,
                        'processed_text': processed_text,
                        'is_toxic': False,
                        'toxicity_probability': 0.0,
                        'confidence': 1.0,
                        'error': '–ü—É—Å—Ç–æ–π TF-IDF –≤–µ–∫—Ç–æ—Ä'
                    }
                    
            else:
                print("‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞!")
                return {
                    'text': original_text,
                    'processed_text': processed_text,
                    'is_toxic': False,
                    'toxicity_probability': 0.0,
                    'confidence': 1.0,
                    'error': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞'
                }

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: {e}")
            return {
                'text': original_text,
                'processed_text': processed_text,
                'is_toxic': False,
                'toxicity_probability': 0.0,
                'confidence': 1.0,
                'error': f'–û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: {str(e)}'
            }

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –≤ –ø–ª–æ—Ç–Ω—É—é –¥–ª—è TF-IDF
            if hasattr(input_data, 'toarray'):
                input_data = input_data.toarray()
                
            prediction_result = self.model.predict(input_data, verbose=0)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            if len(prediction_result.shape) == 2 and prediction_result.shape[1] == 1:
                probability = float(prediction_result[0][0])
            elif len(prediction_result.shape) == 1:
                probability = float(prediction_result[0])
            else:
                probability = float(prediction_result.flatten()[0])
                
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω–æ–º [0, 1]
            probability = max(0.0, min(1.0, probability))
            prediction = 1 if probability > 0.5 else 0
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
            return {
                'text': original_text,
                'processed_text': processed_text,
                'is_toxic': False,
                'toxicity_probability': 0.0,
                'confidence': 1.0,
                'error': f'–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}'
            }

        return {
            'text': original_text,
            'processed_text': processed_text,
            'is_toxic': bool(prediction),
            'toxicity_probability': probability,
            'confidence': max(probability, 1 - probability)
        }
    
    def predict_batch(self, texts):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        results = []
        if not texts:
            return results
            
        for text in texts:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ None –∏ –Ω–µ –ø—É—Å—Ç–æ–π
            if text is None:
                results.append({
                    'text': None,
                    'processed_text': '',
                    'is_toxic': False,
                    'toxicity_probability': 0.0,
                    'confidence': 1.0,
                    'error': '–ü—É—Å—Ç–æ–π –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç (None)'
                })
                continue
                
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            text_str = str(text) if not isinstance(text, str) else text
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            if not text_str.strip():
                results.append({
                    'text': text_str,
                    'processed_text': '',
                    'is_toxic': False,
                    'toxicity_probability': 0.0,
                    'confidence': 1.0,
                    'error': '–ü—É—Å—Ç–æ–π –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç'
                })
                continue
            
            try:
                result = self.predict_toxicity(text_str)
                if result:
                    results.append(result)
                else:
                    # –ï—Å–ª–∏ predict_toxicity –≤–µ—Ä–Ω—É–ª None
                    results.append({
                        'text': text_str,
                        'processed_text': '',
                        'is_toxic': False,
                        'toxicity_probability': 0.0,
                        'confidence': 1.0,
                        'error': '–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–≤–µ—Ä–Ω—É–ª None)'
                    })
            except Exception as e:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ª—é–±—ã–µ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
                print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞ '{text_str[:50]}...': {e}")
                results.append({
                    'text': text_str,
                    'processed_text': '',
                    'is_toxic': False,
                    'toxicity_probability': 0.0,
                    'confidence': 1.0,
                    'error': f'–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}'
                })
        return results

def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å
    predictor = ToxicityPredictor()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    if not predictor.load_model():
        print("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å, –∑–∞–ø—É—Å—Ç–∏–≤ train.py")
        return
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
    test_texts = [
        "$—É–∫@?",
        "–¢—ã –¥–µ–±–∏–ª –∏ –∏–¥–∏–æ—Ç!",
        "–°–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–º–æ—â—å",
        "–£–±–∏–π—Ü–∞ –∏ –º–∞–Ω—å—è–∫!",
        "—Å—É–∫–∞–±–ª—è–¥—å",
        "–ò–¥–∏ –Ω–∞—Ö—É–π!",
        "–µ–±–∞–∞–∞–∞–∞—Ç—å –∫–∞–∫–æ–π –¥–µ–±–∏–ª!!!",
        "–°—É–∫–∞ –±–ª—è–¥—å!",
        "–°–Ω–∞—á–∞–ª–∞ –ø–æ–º–æ–≥–∏ —Å–µ–±–µ —Å–∞–º, –∞ –ø–æ—Ç–æ–º –¥—Ä—É–≥–∏–º. –ü–æ—ç—Ç–æ–º—É —è –±–ª –Ω–µ –ø–æ–º–æ–≥—É —Ç–µ–±–µ. –ü–æ–Ω—è–ª –º–µ–Ω—è, –ø–∞–¥–ª–∞?"
    ]
    
    print("\n" + "="*60)
    print("–ü–†–û–í–ï–†–ö–ê –¢–û–ö–°–ò–ß–ù–û–°–¢–ò –¢–ï–ö–°–¢–û–í")
    print("="*60)
    
    for text in test_texts:
        result = predictor.predict_toxicity(text)
        if result:
            status = "üî¥ –¢–û–ö–°–ò–ß–ù–û" if result['is_toxic'] else "üü¢ –ù–ï–¢–û–ö–°–ò–ß–ù–û"
            print(f"\n–¢–µ–∫—Å—Ç: '{result['text']}'")
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {status}")
            print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏: {result['toxicity_probability']}")
            print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']}")
            print("-" * 40)

if __name__ == "__main__":
    main()

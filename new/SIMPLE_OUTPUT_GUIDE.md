# Simple Output Format Guide

## üéØ Two-Column Output Format

Your toxicity detector now supports a clean, simple output format with just two columns:

- **`id`**: The unique identifier from your input data
- **`label`**: Binary classification (1 = toxic, 0 = non-toxic)

## üöÄ Usage Options

### Option 1: Using the Simple Classifier Script

```bash
python simple_toxicity_classifier.py input.csv output.csv
```

**Features:**
- Always outputs in `id,label` format
- Automatic preprocessing and optimization
- Simplified statistics
- Clean command-line interface

### Option 2: Using the Full Processor with Simple Output Flag

```bash
python improved_csv_processor.py input.csv -o output.csv --simple-output
```

**Features:**
- Full control over all parameters
- Detailed statistics and method tracking
- Can switch between simple and full output formats

## üìä Example Output

**Input CSV:**
```csv
id,text,source
1,"–°–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–º–æ—â—å, –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è!",test
2,–¢—ã –¥—É—Ä–∞–∫ –∏ –∏–¥–∏–æ—Ç!,test
3,"–•–æ—Ä–æ—à–∞—è —Ä–∞–±–æ—Ç–∞, –ø—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –≤ —Ç–æ–º –∂–µ –¥—É—Ö–µ",test
4,"–°—É–∫–∞ –±–ª—è–¥—å, —á—Ç–æ –∑–∞ —Ö–µ—Ä–Ω—è?!",test
```

**Output CSV (Simple Format):**
```csv
id,label
1,0
2,1
3,0
4,1
```

## üîß Command Line Options

### Simple Classifier
```bash
python simple_toxicity_classifier.py [options] input_file output_file

Options:
  --text-column TEXT      Column name with text data (default: text)
  --id-column ID          Column name with ID data (default: id)
  --model MODEL          Path to model file (default: model_tf.h5)
  --tokenizer TOKENIZER  Path to tokenizer file (default: tokenizer_tf.pkl)
  --batch-size N         Batch size for processing (default: 50)
  --no-progress          Disable progress bar
```

### Full Processor with Simple Output
```bash
python improved_csv_processor.py [options] input_file

Options:
  -o, --output OUTPUT    Output CSV file
  --simple-output        Enable simple id,label format
  --id-column ID         Column name with ID data (default: id)
  -c, --column TEXT      Column name with text data (default: text)
  --batch-size N         Batch size for processing (default: 50)
  --no-preprocessing     Disable text preprocessing
  --no-lemmatization     Disable lemmatization
  --no-progress          Disable progress bar
```

## üìà Performance Benefits

- **Clean Output**: Only essential columns (id, label)
- **Fast Processing**: Optimized for bulk classification
- **Memory Efficient**: Minimal output storage
- **Easy Integration**: Standard format for ML pipelines

## üéÆ Quick Examples

### Basic Usage
```bash
# Simple classification
python simple_toxicity_classifier.py data.csv results.csv

# With custom columns
python simple_toxicity_classifier.py data.csv results.csv --text-column "comment" --id-column "post_id"
```

### Advanced Usage
```bash
# Full processor with simple output
python improved_csv_processor.py data.csv -o results.csv --simple-output

# Custom ID column and larger batches
python improved_csv_processor.py data.csv -o results.csv --simple-output --id-column "comment_id" --batch-size 100
```

## üìã Input Requirements

Your input CSV must have:
1. **Text column** (default: "text") - contains the text to classify
2. **ID column** (default: "id") - unique identifier for each row

## ‚úÖ Output Guarantee

The output will always be a valid CSV with exactly two columns:
- `id`: Preserves your original IDs
- `label`: Binary classification (1 = toxic, 0 = non-toxic)

## üîç Label Definitions

- **`1` (Toxic)**: Text contains toxic content including:
  - Profanity and obscenities
  - Insults and personal attacks
  - Hate speech
  - Obfuscated toxic words (–µ.g., "—Å.—É.–∫.–∞", "$—É–∫@")

- **`0` (Non-toxic)**: Text is clean and appropriate:
  - Normal conversation
  - Helpful information
  - Neutral or positive content

## üõ†Ô∏è Troubleshooting

**Common Issues:**

1. **Missing ID column**: Ensure your CSV has the specified ID column
2. **Missing text column**: Ensure your CSV has the specified text column
3. **Model files not found**: Check that `model_tf.h5` and `tokenizer_tf.pkl` exist

**Error Messages:**
- `‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'id' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ñ–∞–π–ª–µ!` - Check your ID column name
- `‚ùå –ö–æ–ª–æ–Ω–∫–∞ 'text' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ñ–∞–π–ª–µ!` - Check your text column name
- `‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ model_tf.h5 –Ω–µ –Ω–∞–π–¥–µ–Ω!` - Check model file path

## üîÆ Integration Examples

### Python Integration
```python
from improved_csv_processor import ImprovedCSVToxicityProcessor

processor = ImprovedCSVToxicityProcessor()
processor.load_model()

stats = processor.process_csv_file(
    input_file='data.csv',
    output_file='results.csv',
    simple_output=True  # Key parameter for simple format
)
```

### Batch Processing
```bash
# Process multiple files
for file in *.csv; do
    python simple_toxicity_classifier.py "$file" "${file%.csv}_results.csv"
done
```

This simple output format makes it easy to integrate toxicity detection into your data processing pipelines!

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Simple toxicity classifier that outputs id,label format.
This is a simplified wrapper around the improved processor for easy use.
"""

import argparse
import sys
from pathlib import Path
from improved_csv_processor import ImprovedCSVToxicityProcessor

def main():
    """
    Simple command-line interface for toxicity classification.
    Always outputs in id,label format where 1=toxic, 0=non-toxic.
    """
    parser = argparse.ArgumentParser(
        description="–ü—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ - –≤—ã–≤–æ–¥–∏—Ç id,label (1=—Ç–æ–∫—Å–∏—á–Ω–æ, 0=–Ω–µ—Ç–æ–∫—Å–∏—á–Ω–æ)"
    )
    parser.add_argument(
        'input_file',
        help='–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É CSV —Ñ–∞–π–ª—É'
    )
    parser.add_argument(
        'output_file',
        help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É CSV —Ñ–∞–π–ª—É'
    )
    parser.add_argument(
        '--text-column',
        default='text',
        help='–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: text)'
    )
    parser.add_argument(
        '--id-column',
        default='id',
        help='–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å ID (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: id)'
    )
    parser.add_argument(
        '--model',
        default='model_tf.h5',
        help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: model_tf.h5)'
    )
    parser.add_argument(
        '--tokenizer',
        default='tokenizer_tf.pkl',
        help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: tokenizer_tf.pkl)'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=50,
        help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 50)'
    )
    parser.add_argument(
        '--no-progress',
        action='store_true',
        help='–û—Ç–∫–ª—é—á–∏—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞'
    )

    args = parser.parse_args()

    # Check if input file exists
    if not Path(args.input_file).exists():
        print(f"‚ùå –§–∞–π–ª {args.input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return 1

    # Check if model files exist
    if not Path(args.model).exists():
        print(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ {args.model} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return 1
    
    if not Path(args.tokenizer).exists():
        print(f"‚ùå –§–∞–π–ª —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ {args.tokenizer} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return 1

    print("üî• –ü–†–û–°–¢–û–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† –¢–û–ö–°–ò–ß–ù–û–°–¢–ò")
    print("="*50)
    print(f"–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {args.input_file}")
    print(f"–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {args.output_file}")
    print(f"–ö–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º: {args.text_column}")
    print(f"–ö–æ–ª–æ–Ω–∫–∞ —Å ID: {args.id_column}")
    print("–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞: id,label (1=—Ç–æ–∫—Å–∏—á–Ω–æ, 0=–Ω–µ—Ç–æ–∫—Å–∏—á–Ω–æ)")
    print("="*50)

    # Initialize processor
    processor = ImprovedCSVToxicityProcessor(
        model_path=args.model,
        tokenizer_path=args.tokenizer,
        use_preprocessing=True,  # Always use preprocessing
        use_lemmatization=True,  # Always use lemmatization
        batch_size=args.batch_size
    )

    # Load model
    if not processor.load_model():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å!")
        return 1

    # Process CSV file with simple output
    stats = processor.process_csv_file(
        input_file=args.input_file,
        output_file=args.output_file,
        text_column=args.text_column,
        id_column=args.id_column,
        simple_output=True,  # Always use simple output
        preserve_columns=False,  # Not needed for simple output
        show_progress=not args.no_progress
    )

    if stats:
        # Print simplified statistics
        print("\n" + "="*50)
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò")
        print("="*50)
        print(f"–í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: {stats['total_texts']}")
        print(f"–¢–æ–∫—Å–∏—á–Ω—ã—Ö (label=1): {stats['toxic_count']}")
        print(f"–ù–µ—Ç–æ–∫—Å–∏—á–Ω—ã—Ö (label=0): {stats['non_toxic_count']}")
        print(f"–ü—Ä–æ—Ü–µ–Ω—Ç —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏: {stats['toxic_percentage']:.1f}%")
        print(f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {stats['processing_time']:.1f} —Å–µ–∫")
        print("="*50)
        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {args.output_file}")
        print("üìã –§–æ—Ä–º–∞—Ç: id,label (–≥–¥–µ 1=—Ç–æ–∫—Å–∏—á–Ω–æ, 0=–Ω–µ—Ç–æ–∫—Å–∏—á–Ω–æ)")
        return 0
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞!")
        return 1


if __name__ == "__main__":
    sys.exit(main())

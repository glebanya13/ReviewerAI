#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Improved CSV processor with enhanced toxicity detection and better error handling.
"""

import pandas as pd
import re
import argparse
import sys
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import time
import warnings

# Import our improved toxicity detector
from toxicity_detector import ToxicityPredictor

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Try to import optional dependencies with better error handling
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    
    # Create a simple progress indicator
    class tqdm:
        def __init__(self, iterable, desc="Processing", **kwargs):
            self.iterable = iterable
            self.desc = desc
            self.total = len(iterable) if hasattr(iterable, '__len__') else 0
            self.current = 0
            print(f"{desc}: 0/{self.total}")
            
        def __iter__(self):
            for item in self.iterable:
                self.current += 1
                if self.current % max(1, self.total // 10) == 0:  # Show progress every 10%
                    print(f"{self.desc}: {self.current}/{self.total}")
                yield item


class EnhancedTextPreprocessor:
    """
    Enhanced text preprocessor with improved obfuscation handling
    and better performance.
    """
    
    def __init__(self, use_simple_lemmatization: bool = True):
        """
        Initialize the preprocessor.
        
        Args:
            use_simple_lemmatization: Whether to apply simple lemmatization
        """
        self.use_simple_lemmatization = use_simple_lemmatization
        
        # Character replacements for obfuscation handling
        self.char_replacements = {
            '@': '–∞', '4': '—á', '6': '–±', '3': '–∑', '0': '–æ',
            '1': '–∏', '7': '—Ç', '5': '–ø', '9': '–¥', '8': '–≤',
            'a': '–∞', 'e': '–µ', 'o': '–æ', 'p': '—Ä', 'c': '—Å',
            'y': '—É', 'x': '—Ö', 'k': '–∫', 'm': '–º', 'h': '–Ω',
            't': '—Ç', 'b': '–≤'
        }
        
        # Quick translation table for performance
        self.translation_table = str.maketrans(
            '@4630175980aeopcyxkmhtb',
            '–∞—á–±–∑–æ–∏—Ç–ø–¥–æ–≤–∞–µ–æ—Ä—Å—É—Ö–∫–º–Ω—Ç–≤'
        )
        
        # Simple lemmatization endings
        self.endings = [
            '—ã–π', '–∞—è', '–æ–µ', '—ã–µ', '—ã—Ö', '—ã–º', '—ã–º–∏',
            '–∏–π', '—è—è', '–µ–µ', '–∏–µ', '–∏—Ö', '–∏–º', '–∏–º–∏',
            '–æ–≤', '–æ–≤–∞', '–æ–≤–æ', '–æ–≤—ã', '–æ–≤—ã—Ö', '–æ–≤—ã–º', '–æ–≤—ã–º–∏',
            '–µ–≤', '–µ–≤–∞', '–µ–≤–æ', '–µ–≤—ã', '–µ–≤—ã—Ö', '–µ–≤—ã–º', '–µ–≤—ã–º–∏',
            '—Ç—å', '—Ç–∏', '–ª', '–ª–∞', '–ª–æ', '–ª–∏', '–ª—Å—å', '–ª–∞—Å—å', '–ª–æ—Å—å', '–ª–∏—Å—å',
            '—Å—è', '—Å—å', '–∞—Ç—å', '—è—Ç—å', '–µ—Ç—å', '–∏—Ç—å', '—É—Ç—å', '—ã—Ç—å',
            '–∞—é', '–∞–µ—à—å', '–∞–µ—Ç', '–∞–µ–º', '–∞–µ—Ç–µ', '–∞—é—Ç',
            '—è—é', '—è–µ—à—å', '—è–µ—Ç', '—è–µ–º', '—è–µ—Ç–µ', '—è—é—Ç',
            '–µ—é', '–µ–µ—à—å', '–µ–µ—Ç', '–µ–µ–º', '–µ–µ—Ç–µ', '–µ—é—Ç',
            '–∏—é', '–∏—à—å', '–∏—Ç', '–∏–º', '–∏—Ç–µ', '—è—Ç'
        ]

    def clean_text(self, text: str) -> str:
        """
        Enhanced text cleaning with better obfuscation handling.
        """
        if pd.isna(text) or text == '':
            return ''
        
        text = str(text).lower()
        
        # Remove HTML tags
        text = re.sub(r"<[^>]+>", " ", text)
        
        # Remove dates
        text = re.sub(
            r"(\d{4}[-./]\d{1,2}[-./]\d{1,2})|(\d{1,2}[-./]\d{1,2}[-./]\d{2,4})",
            "",
            text,
        )
        
        # Handle spaced obfuscation (e.g., "—Å —É –∫ –∞")
        text = re.sub(r'(\w)\s+(\w)\s+(\w)', r'\1\2\3', text)
        
        # Handle obfuscation: remove separators within words
        words = text.split()
        processed_words = []
        for word in words:
            if len(word) > 2 and any(sep in word for sep in '-_.'):
                clean_word = re.sub(r'[\-_\.]', '', word)
                processed_words.append(clean_word)
            else:
                processed_words.append(word)
        text = ' '.join(processed_words)
        
        # Character replacements for obfuscation
        text = text.translate(self.translation_table)
        
        # Normalize repeating characters (3+ -> 2)
        text = re.sub(r'(.)\1{2,}', r'\1\1', text)
        
        # Remove emojis and special symbols, keeping letters, digits, punctuation, spaces
        text = re.sub(r"[^\w\s.,!?-]", " ", text, flags=re.UNICODE)
        
        # Remove number sequences of 3+ digits
        text = re.sub(r"\d{3,}", "", text)
        
        # Normalize spaces
        text = re.sub(r"\s+", " ", text).strip()
        
        return text

    def simple_lemmatize(self, text: str) -> str:
        """
        Simple lemmatization with space preservation.
        """
        words = text.split()
        lemmatized = []
        
        for word in words:
            if len(word) < 3:
                lemmatized.append(word)
                continue
            
            # Try to remove endings
            found = False
            for ending in sorted(self.endings, key=len, reverse=True):
                if word.endswith(ending) and len(word) > len(ending) + 1:
                    lemmatized.append(word[:-len(ending)])
                    found = True
                    break
            
            if not found:
                lemmatized.append(word)
        
        return ' '.join(lemmatized)

    def preprocess_text(self, text: str) -> str:
        """
        Complete text preprocessing pipeline.
        """
        if pd.isna(text) or text == '':
            return ''
        
        # 1. Clean text
        cleaned = self.clean_text(text)
        
        # 2. Remove single characters
        words = cleaned.split()
        words = [word for word in words if len(word) > 1]
        cleaned = ' '.join(words)
        
        # 3. Apply simple lemmatization if enabled
        if self.use_simple_lemmatization:
            processed = self.simple_lemmatize(cleaned)
        else:
            processed = cleaned
        
        return processed


class ImprovedCSVToxicityProcessor:
    """
    Improved processor class for handling CSV files with enhanced toxicity detection.
    """
    
    def __init__(
        self,
        model_path: str = 'model_tf.h5',
        tokenizer_path: str = 'tokenizer_tf.pkl',
        use_preprocessing: bool = True,
        use_lemmatization: bool = True,
        batch_size: int = 100
    ):
        """
        Initialize the improved CSV toxicity processor.
        
        Args:
            model_path: Path to the trained model
            tokenizer_path: Path to the tokenizer
            use_preprocessing: Whether to apply advanced preprocessing
            use_lemmatization: Whether to apply lemmatization
            batch_size: Batch size for processing
        """
        self.model_path = model_path
        self.tokenizer_path = tokenizer_path
        self.use_preprocessing = use_preprocessing
        self.batch_size = batch_size
        
        # Initialize components
        self.toxicity_predictor = ToxicityPredictor()
        self.preprocessor = EnhancedTextPreprocessor(
            use_simple_lemmatization=use_lemmatization
        ) if use_preprocessing else None
        
        # Load model
        self.model_loaded = False

    def load_model(self) -> bool:
        """
        Load the toxicity detection model.
        """
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
        success = self.toxicity_predictor.load_model(
            self.model_path, 
            self.tokenizer_path
        )
        self.model_loaded = success
        return success

    def preprocess_texts(self, texts: List[str]) -> List[str]:
        """
        Preprocess a list of texts.
        """
        if not self.use_preprocessing or not self.preprocessor:
            return texts
        
        processed = []
        for text in texts:
            processed_text = self.preprocessor.preprocess_text(text)
            processed.append(processed_text)
        
        return processed

    def process_csv_file(
        self,
        input_file: str,
        output_file: str,
        text_column: str = 'text',
        id_column: str = 'id',
        simple_output: bool = False,
        preserve_columns: bool = True,
        show_progress: bool = True
    ) -> Dict[str, any]:
        """
        Process a CSV file and add improved toxicity predictions.
        
        Args:
            input_file: Path to input CSV file
            output_file: Path to output CSV file
            text_column: Name of the column containing text data
            id_column: Name of the column containing ID data
            simple_output: If True, output only id and label columns (1=toxic, 0=non-toxic)
            preserve_columns: Whether to preserve all original columns (ignored if simple_output=True)
            show_progress: Whether to show progress bar
            
        Returns:
            Dictionary with processing statistics
        """
        if not self.model_loaded:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ load_model()")
            return None
        
        print(f"üìñ –ó–∞–≥—Ä—É–∑–∫–∞ CSV —Ñ–∞–π–ª–∞: {input_file}")
        
        try:
            df = pd.read_csv(input_file)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}")
            return None
        
        if text_column not in df.columns:
            print(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ '{text_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ñ–∞–π–ª–µ!")
            print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
            return None
        
        if simple_output and id_column not in df.columns:
            print(f"‚ùå –ö–æ–ª–æ–Ω–∫–∞ '{id_column}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Ñ–∞–π–ª–µ!")
            print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
            return None
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")
        print(f"üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫—É: '{text_column}'")
        
        # Prepare results
        results = []
        processed_texts = []
        original_texts = []
        total_texts = len(df)
        
        # Process in batches for better performance
        print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ (batch_size={self.batch_size})...")
        
        start_time = time.time()
        
        # Create progress iterator
        batch_range = range(0, total_texts, self.batch_size)
        if show_progress:
            iterator = tqdm(batch_range, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π")
        else:
            iterator = batch_range
        
        for i in iterator:
            batch_texts = df[text_column].iloc[i:i+self.batch_size].tolist()
            original_texts.extend(batch_texts)
            
            # Preprocess if enabled
            if self.use_preprocessing:
                batch_processed = self.preprocess_texts(batch_texts)
                processed_texts.extend(batch_processed)
                # Use processed texts for prediction
                prediction_texts = batch_processed
            else:
                prediction_texts = batch_texts
                processed_texts.extend([''] * len(batch_texts))  # Empty processed texts
            
            # Get toxicity predictions with improved method
            batch_results = self.toxicity_predictor.predict_batch(
                prediction_texts
            )
            results.extend(batch_results)
        
        processing_time = time.time() - start_time
        
        # Prepare prediction results
        is_toxic = [r['is_toxic'] if r else False for r in results]
        toxicity_prob = [r['toxicity_probability'] if r else 0.0 for r in results]
        confidence = [r['confidence'] if r else 0.0 for r in results]
        methods = [r.get('method', 'unknown') if r else 'error' for r in results]
        errors = [r.get('error', '') if r else '–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏' for r in results]
        notes = [r.get('note', '') if r else '' for r in results]
        
        # Create output dataframe based on output format
        if simple_output:
            # Simple output: only id and label columns
            output_df = pd.DataFrame()
            output_df['id'] = df[id_column]
            output_df['label'] = [1 if toxic else 0 for toxic in is_toxic]
        else:
            # Full output: preserve columns and add detailed predictions
            if preserve_columns:
                output_df = df.copy()
            else:
                output_df = pd.DataFrame()
                output_df[text_column] = df[text_column]
            
            # Add preprocessing column if enabled
            if self.use_preprocessing:
                output_df['processed_text'] = processed_texts
            
            # Add improved toxicity prediction columns
            output_df['is_toxic'] = is_toxic
            output_df['toxicity_probability'] = toxicity_prob
            output_df['confidence'] = confidence
            output_df['detection_method'] = methods
            output_df['prediction_error'] = errors
            output_df['notes'] = notes
        
        # Save results
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤: {output_file}")
        try:
            output_df.to_csv(output_file, index=False, encoding='utf-8')
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
            return None
        
        # Calculate enhanced statistics
        toxic_count = sum(is_toxic)
        valid_predictions = sum(1 for r in results if r and 'error' not in r)
        method_counts = {}
        for method in methods:
            method_counts[method] = method_counts.get(method, 0) + 1
        
        stats = {
            'total_texts': total_texts,
            'valid_predictions': valid_predictions,
            'toxic_count': toxic_count,
            'non_toxic_count': valid_predictions - toxic_count,
            'error_count': total_texts - valid_predictions,
            'toxic_percentage': (toxic_count / valid_predictions * 100) if valid_predictions > 0 else 0,
            'processing_time': processing_time,
            'texts_per_second': total_texts / processing_time if processing_time > 0 else 0,
            'method_counts': method_counts
        }
        
        return stats

    def print_statistics(self, stats: Dict[str, any]):
        """
        Print enhanced processing statistics.
        """
        if not stats:
            return
        
        print("\n" + "="*70)
        print("üìä –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò")
        print("="*70)
        print(f"–í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: {stats['total_texts']}")
        print(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['valid_predictions']}")
        print(f"–û—à–∏–±–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {stats['error_count']}")
        print(f"–¢–æ–∫—Å–∏—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {stats['toxic_count']}")
        print(f"–ù–µ—Ç–æ–∫—Å–∏—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {stats['non_toxic_count']}")
        print(f"–ü—Ä–æ—Ü–µ–Ω—Ç —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏: {stats['toxic_percentage']:.2f}%")
        print(f"–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {stats['processing_time']:.2f} —Å–µ–∫")
        print(f"–°–∫–æ—Ä–æ—Å—Ç—å: {stats['texts_per_second']:.2f} —Ç–µ–∫—Å—Ç–æ–≤/—Å–µ–∫")
        
        print("\nüìà –ú–ï–¢–û–î–´ –î–ï–¢–ï–ö–¶–ò–ò:")
        method_emojis = {
            'model': 'ü§ñ –ú–æ–¥–µ–ª—å',
            'pattern_fallback': 'üîç –ü–∞—Ç—Ç–µ—Ä–Ω—ã',
            'combined': 'ü§ñ+üîç –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π',
            'empty_text': '‚ö™ –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç',
            'tokenization_failed': '‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏',
            'padding_failed': '‚ö†Ô∏è –û—à–∏–±–∫–∞ padding',
            'model_failed': '‚ùå –û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏',
            'error_fallback': '‚ùå –û—à–∏–±–∫–∞ —Å fallback',
            'error': '‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞',
            'unknown': '‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
        }
        
        for method, count in stats['method_counts'].items():
            emoji_method = method_emojis.get(method, f"‚ùì {method}")
            percentage = (count / stats['total_texts'] * 100) if stats['total_texts'] > 0 else 0
            print(f"  {emoji_method}: {count} ({percentage:.1f}%)")
        
        print("="*70)


def main():
    """
    Main function with improved command-line interface.
    """
    parser = argparse.ArgumentParser(
        description="–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ CSV —Ñ–∞–π–ª–æ–≤ —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞"
    )
    parser.add_argument(
        'input_file',
        help='–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É CSV —Ñ–∞–π–ª—É'
    )
    parser.add_argument(
        '-o', '--output',
        default='improved_output_with_toxicity.csv',
        help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É CSV —Ñ–∞–π–ª—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: improved_output_with_toxicity.csv)'
    )
    parser.add_argument(
        '-c', '--column',
        default='text',
        help='–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: text)'
    )
    parser.add_argument(
        '--id-column',
        default='id',
        help='–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å ID (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: id)'
    )
    parser.add_argument(
        '--simple-output',
        action='store_true',
        help='–í—ã–≤–æ–¥–∏—Ç—å —Ç–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫–∏ id –∏ label (1=—Ç–æ–∫—Å–∏—á–Ω–æ, 0=–Ω–µ—Ç–æ–∫—Å–∏—á–Ω–æ)'
    )
    parser.add_argument(
        '--model',
        default='model_tf.h5',
        help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: model_tf.h5)'
    )
    parser.add_argument(
        '--tokenizer',
        default='tokenizer_tf.pkl',
        help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: tokenizer_tf.pkl)'
    )
    parser.add_argument(
        '--no-preprocessing',
        action='store_true',
        help='–û—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞'
    )
    parser.add_argument(
        '--no-lemmatization',
        action='store_true',
        help='–û—Ç–∫–ª—é—á–∏—Ç—å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=50,
        help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 50)'
    )
    parser.add_argument(
        '--preserve-columns',
        action='store_true',
        default=True,
        help='–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –≤–∫–ª—é—á–µ–Ω–æ)'
    )
    parser.add_argument(
        '--no-progress',
        action='store_true',
        help='–û—Ç–∫–ª—é—á–∏—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞'
    )

    args = parser.parse_args()

    # Check if input file exists
    if not Path(args.input_file).exists():
        print(f"‚ùå –§–∞–π–ª {args.input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return 1

    # Check if model files exist
    if not Path(args.model).exists():
        print(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ {args.model} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return 1
    
    if not Path(args.tokenizer).exists():
        print(f"‚ùå –§–∞–π–ª —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ {args.tokenizer} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return 1

    print("üöÄ –ó–ê–ü–£–°–ö –£–õ–£–ß–®–ï–ù–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–ò CSV –° –î–ï–¢–ï–ö–¶–ò–ï–ô –¢–û–ö–°–ò–ß–ù–û–°–¢–ò")
    print("="*70)
    print(f"–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {args.input_file}")
    print(f"–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {args.output}")
    print(f"–ö–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º: {args.column}")
    if args.simple_output:
        print(f"–ö–æ–ª–æ–Ω–∫–∞ —Å ID: {args.id_column}")
        print("–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞: –ü—Ä–æ—Å—Ç–æ–π (id, label)")
    else:
        print("–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π")
    print(f"–ú–æ–¥–µ–ª—å: {args.model}")
    print(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {args.tokenizer}")
    print(f"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: {'–≤—ã–∫–ª—é—á–µ–Ω–∞' if args.no_preprocessing else '–≤–∫–ª—é—á–µ–Ω–∞'}")
    print(f"–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è: {'–≤—ã–∫–ª—é—á–µ–Ω–∞' if args.no_lemmatization else '–≤–∫–ª—é—á–µ–Ω–∞'}")
    print(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {args.batch_size}")
    print(f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {'–≤—ã–∫–ª—é—á–µ–Ω' if args.no_progress else '–≤–∫–ª—é—á–µ–Ω'}")
    print("="*70)

    # Initialize improved processor
    processor = ImprovedCSVToxicityProcessor(
        model_path=args.model,
        tokenizer_path=args.tokenizer,
        use_preprocessing=not args.no_preprocessing,
        use_lemmatization=not args.no_lemmatization,
        batch_size=args.batch_size
    )

    # Load model
    if not processor.load_model():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å!")
        return 1

    # Process CSV file
    stats = processor.process_csv_file(
        input_file=args.input_file,
        output_file=args.output,
        text_column=args.column,
        id_column=args.id_column,
        simple_output=args.simple_output,
        preserve_columns=args.preserve_columns,
        show_progress=not args.no_progress
    )

    if stats:
        processor.print_statistics(stats)
        print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {args.output}")
        return 0
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞!")
        return 1


if __name__ == "__main__":
    sys.exit(main())

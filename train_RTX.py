import pandas as pd
import numpy as np
import time
import pickle
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_recall_curve, roc_auc_score
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
import psutil
from datetime import datetime
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# TensorFlow —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è RTX 4060
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.mixed_precision import set_global_policy
from tensorflow.keras.regularizers import l2
from tensorflow.keras import backend as K

# –ö–∞—Å—Ç–æ–º–Ω–∞—è F1-–º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
def f1_metric(y_true, y_pred):
    """–ö–∞—Å—Ç–æ–º–Ω–∞—è F1-–º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è TensorFlow"""
    y_pred = tf.cast(tf.greater(y_pred, 0.5), tf.float32)
    y_true = tf.cast(y_true, tf.float32)

    tp = tf.reduce_sum(y_true * y_pred)
    fp = tf.reduce_sum((1 - y_true) * y_pred)
    fn = tf.reduce_sum(y_true * (1 - y_pred))

    precision = tp / (tp + fp + K.epsilon())
    recall = tp / (tp + fn + K.epsilon())

    return 2 * precision * recall / (precision + recall + K.epsilon())

class RTXModelTrainer:
    """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è RTX 4060"""

    def __init__(self):
        self.setup_gpu()
        self.setup_mixed_precision()
        self.vectorizer = None
        self.model = None
        self.history = None
        self.results = {}

    def setup_gpu(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ RTX 4060"""
        print("üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤...")

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CPU –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
        os.environ['OMP_NUM_THREADS'] = str(psutil.cpu_count())
        os.environ['TF_NUM_INTEROP_THREADS'] = str(psutil.cpu_count())
        os.environ['TF_NUM_INTRAOP_THREADS'] = str(psutil.cpu_count())

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
        gpus = tf.config.list_physical_devices('GPU')
        if gpus:
            try:
                # –í–∫–ª—é—á–µ–Ω–∏–µ —Ä–æ—Å—Ç–∞ –ø–∞–º—è—Ç–∏ GPU
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)

                print(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ GPU: {len(gpus)}")
                for i, gpu in enumerate(gpus):
                    try:
                        details = tf.config.experimental.get_device_details(gpu)
                        print(f"   GPU {i}: {details.get('device_name', 'RTX 4060')}")
                    except:
                        print(f"   GPU {i}: RTX 4060 (GPU –∞–∫—Ç–∏–≤–µ–Ω)")

            except RuntimeError as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ GPU: {e}")
                print("   –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π CPU —Ä–µ–∂–∏–º...")
                gpus = []
        else:
            print("‚ö†Ô∏è GPU –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω TensorFlow, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π CPU")

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
        tf.config.threading.set_inter_op_parallelism_threads(psutil.cpu_count())
        tf.config.threading.set_intra_op_parallelism_threads(psutil.cpu_count())

        print(f"‚úÖ CPU —è–¥–µ—Ä: {psutil.cpu_count()}")
        print(f"‚úÖ RAM: {psutil.virtual_memory().total / (1024**3):.1f} –ì–ë")

        return len(gpus) > 0

    def setup_mixed_precision(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –Ω–∞ RTX 4060"""
        try:
            set_global_policy('mixed_float16')
            print("‚úÖ –í–∫–ª—é—á–µ–Ω–∞ —Å–º–µ—à–∞–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (mixed_float16)")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å —Å–º–µ—à–∞–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å: {e}")

    def load_and_prepare_data(self, data_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = pd.read_csv(data_path)
        print(f"   –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}")
        print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {df['label'].value_counts().to_dict()}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—Å–∫–∏
        if df['text'].isnull().sum() > 0:
            print(f"   –£–¥–∞–ª–µ–Ω–∏–µ {df['text'].isnull().sum()} —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏")
            df = df.dropna(subset=['text'])

        X = df['text'].astype(str)
        y = df['label'].astype(int)

        return X, y

    def create_tfidf_features(self, X_train, X_test, max_features=8000):  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 15000 –¥–æ 8000
        """–°–æ–∑–¥–∞–Ω–∏–µ TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏"""
        print("üî§ –°–æ–∑–¥–∞–Ω–∏–µ TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

        self.vectorizer = TfidfVectorizer(
            max_features=max_features,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            ngram_range=(1, 2),  # –¢–æ–ª—å–∫–æ —É–Ω–∏–≥—Ä–∞–º–º—ã –∏ –±–∏–≥—Ä–∞–º–º—ã
            min_df=5,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤
            max_df=0.85,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤
            sublinear_tf=True,
            use_idf=True,
            smooth_idf=True,
            stop_words=None,  # –î–∞–Ω–Ω—ã–µ —É–∂–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
            dtype=np.float32  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float32 –≤–º–µ—Å—Ç–æ float64
        )

        print("   –û–±—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞...")
        X_train_tfidf = self.vectorizer.fit_transform(X_train)
        X_test_tfidf = self.vectorizer.transform(X_test)

        print(f"   –†–∞–∑–º–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞: {X_train_tfidf.shape[1]}")

        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü
        memory_size = (X_train_tfidf.data.nbytes + X_train_tfidf.indices.nbytes + X_train_tfidf.indptr.nbytes) / (1024**3)
        print(f"   –†–∞–∑–º–µ—Ä –≤ –ø–∞–º—è—Ç–∏: {memory_size:.2f} –ì–ë")

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        return X_train_tfidf, X_test_tfidf

    def apply_smote(self, X_train, y_train):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤ —Å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–º–∏ –º–∞—Ç—Ä–∏—Ü–∞–º–∏"""
        print("‚öñÔ∏è –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE...")

        unique, counts = np.unique(y_train, return_counts=True)
        print(f"   –î–æ SMOTE: {dict(zip(unique, counts))}")

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –≤ –ø–ª–æ—Ç–Ω—É—é —Ç–æ–ª—å–∫–æ –¥–ª—è SMOTE
        print("   –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è SMOTE...")
        X_train_dense = X_train.toarray() if hasattr(X_train, 'toarray') else X_train

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ SMOTE –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
        smote = SMOTE(
            sampling_strategy='auto',
            k_neighbors=3,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            random_state=42
        )

        X_train_balanced, y_train_balanced = smote.fit_resample(X_train_dense, y_train)

        unique, counts = np.unique(y_train_balanced, return_counts=True)
        print(f"   –ü–æ—Å–ª–µ SMOTE: {dict(zip(unique, counts))}")

        return X_train_balanced, y_train_balanced

    def apply_smote_batched(self, X_train, y_train, batch_size=10000):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE —Å –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏"""
        print("‚öñÔ∏è –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE —Å –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π...")

        unique, counts = np.unique(y_train, return_counts=True)
        print(f"   –î–æ SMOTE: {dict(zip(unique, counts))}")

        # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–º–Ω–æ–≥–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π SMOTE
        if X_train.shape[0] <= 20000:
            print("   –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±—ã—á–Ω–æ–≥–æ SMOTE...")
            X_train_dense = X_train.toarray()

            smote = SMOTE(
                sampling_strategy='auto',
                k_neighbors=3,
                random_state=42
            )

            X_train_balanced, y_train_balanced = smote.fit_resample(X_train_dense, y_train)
        else:
            # –î–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            print("   –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏...")

            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
            class_0_indices = np.where(y_train == 0)[0]
            class_1_indices = np.where(y_train == 1)[0]

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –º–µ–Ω—å—à–µ–≥–æ –∫–ª–∞—Å—Å–∞
            min_class_size = min(len(class_0_indices), len(class_1_indices))
            target_size = min_class_size * 2  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤ 2 —Ä–∞–∑–∞

            print(f"   –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞: {target_size}")

            # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
            np.random.seed(42)

            if len(class_0_indices) < target_size:
                # –î—É–±–ª–∏—Ä—É–µ–º –∫–ª–∞—Å—Å 0
                additional_needed = target_size - len(class_0_indices)
                additional_indices = np.random.choice(class_0_indices, additional_needed, replace=True)
                balanced_0_indices = np.concatenate([class_0_indices, additional_indices])
            else:
                balanced_0_indices = np.random.choice(class_0_indices, target_size, replace=False)

            if len(class_1_indices) < target_size:
                # –î—É–±–ª–∏—Ä—É–µ–º –∫–ª–∞—Å—Å 1
                additional_needed = target_size - len(class_1_indices)
                additional_indices = np.random.choice(class_1_indices, additional_needed, replace=True)
                balanced_1_indices = np.concatenate([class_1_indices, additional_indices])
            else:
                balanced_1_indices = np.random.choice(class_1_indices, target_size, replace=False)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å—ã
            all_indices = np.concatenate([balanced_0_indices, balanced_1_indices])
        return X_train_balanced, y_train_balanced

    def create_model(self, input_dim):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è RTX 4060"""
        print("üß† –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")

        model = Sequential([
            Input(shape=(input_dim,), name='input_layer'),

            # –ü–µ—Ä–≤—ã–π –±–ª–æ–∫
            Dense(1024, activation='relu', kernel_regularizer=l2(0.001), name='dense_1'),
            BatchNormalization(name='bn_1'),
            Dropout(0.3, name='dropout_1'),

            # –í—Ç–æ—Ä–æ–π –±–ª–æ–∫
            Dense(512, activation='relu', kernel_regularizer=l2(0.001), name='dense_2'),
            BatchNormalization(name='bn_2'),
            Dropout(0.4, name='dropout_2'),

            # –¢—Ä–µ—Ç–∏–π –±–ª–æ–∫
            Dense(256, activation='relu', kernel_regularizer=l2(0.001), name='dense_3'),
            BatchNormalization(name='bn_3'),
            Dropout(0.3, name='dropout_3'),

            # –ß–µ—Ç–≤–µ—Ä—Ç—ã–π –±–ª–æ–∫
            Dense(128, activation='relu', kernel_regularizer=l2(0.001), name='dense_4'),
            BatchNormalization(name='bn_4'),
            Dropout(0.2, name='dropout_4'),

            # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
            Dense(1, activation='sigmoid', dtype='float32', name='output')
        ])

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è RTX 4060
        optimizer = Adam(
            learning_rate=0.001,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-07,
            clipnorm=1.0
        )

        model.compile(
            optimizer=optimizer,
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall', f1_metric]
        )

        print("   –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
        model.summary()

        return model

    def create_callbacks(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ callback'–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        callbacks = [
            EarlyStopping(
                monitor='val_f1_metric',  # –ú–æ–Ω–∏—Ç–æ—Ä–∏–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π F1-score
                patience=15,
                restore_best_weights=True,
                verbose=1,
                mode='max'  # –ú–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º F1-score
            ),
            ReduceLROnPlateau(
                monitor='val_f1_metric',
                factor=0.5,
                patience=7,
                min_lr=1e-7,
                verbose=1,
                mode='max'
            ),
            ModelCheckpoint(
                filepath='model_tf_31_best.h5',
                monitor='val_f1_metric',  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ F1-score
                save_best_only=True,
                save_weights_only=False,
                verbose=1,
                mode='max'
            )
        ]

        return callbacks

    def train_model(self, X_train, y_train, X_val, y_val):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        print("üèãÔ∏è –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")

        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model = self.create_model(X_train.shape[1])

        # Callback'–∏
        callbacks = self.create_callbacks()

        start_time = time.time()

        # –û–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=100,
            batch_size=512,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            callbacks=callbacks,
            verbose=1,
            shuffle=True
        )

        training_time = time.time() - start_time
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.2f} —Å–µ–∫—É–Ω–¥")

        return training_time

    def evaluate_model(self, X_test, y_test):
        """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"""
        print("üìä –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_pred_proba = self.model.predict(X_test, batch_size=1024, verbose=1)
        y_pred = (y_pred_proba > 0.5).astype(int).flatten()

        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_pred_proba)

        self.results = {
            'accuracy': accuracy,
            'f1_score': f1,
            'roc_auc': roc_auc,
            'classification_report': classification_report(y_test, y_pred),
            'confusion_matrix': confusion_matrix(y_test, y_pred),
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba
        }

        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.4f}")
        print(f"‚úÖ F1-score: {f1:.4f}")
        print(f"‚úÖ ROC-AUC: {roc_auc:.4f}")

        return self.results

    def save_model_and_results(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model.save('model_tf_31.h5')
        print("   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: model_tf_31.h5")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
        with open('tokenizer_tf_31.pkl', 'wb') as f:
            pickle.dump(self.vectorizer, f)
        print("   ‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: tokenizer_tf_31.pkl")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open('results_tf_31.pkl', 'wb') as f:
            pickle.dump(self.results, f)
        print("   ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: results_tf_31.pkl")

    def plot_training_history(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
        print("üìà –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤...")

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 2x3 –¥–ª—è F1-score

        # –¢–æ—á–Ω–æ—Å—Ç—å
        axes[0, 0].plot(self.history.history['accuracy'], label='–û–±—É—á–µ–Ω–∏–µ', linewidth=2)
        axes[0, 0].plot(self.history.history['val_accuracy'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è', linewidth=2)
        axes[0, 0].set_title('–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[0, 0].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # –§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        axes[0, 1].plot(self.history.history['loss'], label='–û–±—É—á–µ–Ω–∏–µ', linewidth=2)
        axes[0, 1].plot(self.history.history['val_loss'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è', linewidth=2)
        axes[0, 1].set_title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[0, 1].set_ylabel('–ü–æ—Ç–µ—Ä–∏')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)

        # F1-score - –æ—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        if 'f1_metric' in self.history.history:
            axes[0, 2].plot(self.history.history['f1_metric'], label='F1 –û–±—É—á–µ–Ω–∏–µ', linewidth=2, color='green')
            axes[0, 2].plot(self.history.history['val_f1_metric'], label='F1 –í–∞–ª–∏–¥–∞—Ü–∏—è', linewidth=2, color='red')
            axes[0, 2].set_title('F1-Score', fontsize=14, fontweight='bold')
            axes[0, 2].set_xlabel('–≠–ø–æ—Ö–∞')
            axes[0, 2].set_ylabel('F1-Score')
            axes[0, 2].legend()
            axes[0, 2].grid(True, alpha=0.3)

            # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º F1-score
            max_val_f1 = max(self.history.history['val_f1_metric'])
            max_epoch = self.history.history['val_f1_metric'].index(max_val_f1)
            axes[0, 2].annotate(f'Max F1: {max_val_f1:.4f}\n(–≠–ø–æ—Ö–∞ {max_epoch+1})',
                              xy=(max_epoch, max_val_f1),
                              xytext=(10, 10),
                              textcoords='offset points',
                              bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                              arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))

        # Precision
        if 'precision' in self.history.history:
            axes[1, 0].plot(self.history.history['precision'], label='–û–±—É—á–µ–Ω–∏–µ', linewidth=2)
            axes[1, 0].plot(self.history.history['val_precision'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è', linewidth=2)
            axes[1, 0].set_title('Precision', fontsize=14, fontweight='bold')
            axes[1, 0].set_xlabel('–≠–ø–æ—Ö–∞')
            axes[1, 0].set_ylabel('Precision')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

        # Recall
        if 'recall' in self.history.history:
            axes[1, 1].plot(self.history.history['recall'], label='–û–±—É—á–µ–Ω–∏–µ', linewidth=2)
            axes[1, 1].plot(self.history.history['val_recall'], label='–í–∞–ª–∏–¥–∞—Ü–∏—è', linewidth=2)
            axes[1, 1].set_title('Recall', fontsize=14, fontweight='bold')
            axes[1, 1].set_xlabel('–≠–ø–æ—Ö–∞')
            axes[1, 1].set_ylabel('Recall')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)

        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        axes[1, 2].plot(self.history.history['val_accuracy'], label='Accuracy', linewidth=2)
        if 'val_precision' in self.history.history:
            axes[1, 2].plot(self.history.history['val_precision'], label='Precision', linewidth=2)
        if 'val_recall' in self.history.history:
            axes[1, 2].plot(self.history.history['val_recall'], label='Recall', linewidth=2)
        if 'val_f1_metric' in self.history.history:
            axes[1, 2].plot(self.history.history['val_f1_metric'], label='F1-Score', linewidth=3, color='red')
        axes[1, 2].set_title('–í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ (–í–∞–ª–∏–¥–∞—Ü–∏—è)', fontsize=14, fontweight='bold')
        axes[1, 2].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[1, 2].set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('training_history_tf_31.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("   ‚úÖ –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω: training_history_tf_31.png")

        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É F1-score
        if 'f1_metric' in self.history.history:
            print(f"   üìä –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π F1-score –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {max(self.history.history['val_f1_metric']):.4f}")
            print(f"   üìä –§–∏–Ω–∞–ª—å–Ω—ã–π F1-score –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {self.history.history['val_f1_metric'][-1]:.4f}")
            print(f"   üìä –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π F1-score –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏: {max(self.history.history['f1_metric']):.4f}")

    def plot_confusion_matrix(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫"""
        plt.figure(figsize=(8, 6))
        cm = self.results['confusion_matrix']
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫')
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
        plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è')
        plt.savefig('confusion_matrix_tf_31.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("   ‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: confusion_matrix_tf_31.png")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    print("üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –Ω–∞ RTX 4060")
    print("=" * 60)

    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
    trainer = RTXModelTrainer()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X, y = trainer.load_and_prepare_data('C:/Andrey/Study/5 —Å–µ–º/HAKATON/HAKATON/dataset/train_final_complete.csv')

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    print("üîÑ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
    )

    print(f"   –û–±—É—á–µ–Ω–∏–µ: {len(X_train)}")
    print(f"   –í–∞–ª–∏–¥–∞—Ü–∏—è: {len(X_val)}")
    print(f"   –¢–µ—Å—Ç: {len(X_test)}")

    # –°–æ–∑–¥–∞–Ω–∏–µ TF-IDF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_train_tfidf, X_val_tfidf = trainer.create_tfidf_features(X_train, X_val)
    X_test_tfidf = trainer.vectorizer.transform(X_test)

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
    print("üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    X_val_dense = X_val_tfidf.toarray()

    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SMOTE
    X_train_balanced, y_train_balanced = trainer.apply_smote(X_train_tfidf, y_train)

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    training_time = trainer.train_model(X_train_balanced, y_train_balanced, X_val_dense, y_val)

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –ø–ª–æ—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    print("üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    X_test_dense = X_test_tfidf.toarray()

    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    results = trainer.evaluate_model(X_test_dense, y_test)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    trainer.save_model_and_results()

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    trainer.plot_training_history()
    trainer.plot_confusion_matrix()

    # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "=" * 60)
    print("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("=" * 60)
    print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.2f} —Å–µ–∫—É–Ω–¥")
    print(f"üéØ –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {results['accuracy']:.4f}")
    print(f"üìä F1-score: {results['f1_score']:.4f}")
    print(f"üìà ROC-AUC: {results['roc_auc']:.4f}")
    print("\nüìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print("   ‚Ä¢ model_tf_31.h5 - TensorFlow –º–æ–¥–µ–ª—å")
    print("   ‚Ä¢ tokenizer_tf_31.pkl - TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä")
    print("   ‚Ä¢ results_tf_31.pkl - –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è")
    print("   ‚Ä¢ confusion_matrix_tf_31.png - –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫")
    print("   ‚Ä¢ training_history_tf_31.png - –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è")
    print("=" * 60)

if __name__ == "__main__":
    main()
